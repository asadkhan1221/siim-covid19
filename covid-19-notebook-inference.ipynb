{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setting Up the Environment","metadata":{}},{"cell_type":"code","source":"!conda install '/kaggle/input/pydicom-conda-helper/libjpeg-turbo-2.1.0-h7f98852_0.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/libgcc-ng-9.3.0-h2828fa1_19.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/gdcm-2.8.9-py37h500ead1_1.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/conda-4.10.1-py37h89c1867_0.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/certifi-2020.12.5-py37h89c1867_1.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/openssl-1.1.1k-h7f98852_0.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/efficientnet-1.0.0-py37h06a4308_0.tar.bz2' -y\n!conda install '/kaggle/input/pydicom-conda-helper/keras-applications-1.0.8-py_1.tar.bz2' -c conda-forge -y","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:20:28.641119Z","iopub.execute_input":"2021-08-15T18:20:28.641561Z","iopub.status.idle":"2021-08-15T18:21:57.719199Z","shell.execute_reply.started":"2021-08-15T18:20:28.641463Z","shell.execute_reply":"2021-08-15T18:21:57.718243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making Necessary Directories and Setting Paths","metadata":{}},{"cell_type":"code","source":"!mkdir -p ./test_images_rgb/\n!mkdir -p ./test_images_psuedo_color/\n!mkdir -p ./model_1_outputs/\n!mkdir -p ./model_2_outputs/\n!mkdir -p ./model_3_outputs/\n!mkdir -p ./model_4_outputs/\n!mkdir -p ./model_5_outputs/\n!mkdir -p ./model_6_outputs/\n\n\nrgb_test_images_path = './test_images_rgb/'\npsuedo_color_test_images_path = './test_images_psuedo_color/'\nmodel_1_outputs_path = './model_1_outputs/'\nmodel_2_outputs_path = './model_2_outputs/'\nmodel_3_outputs_path = './model_3_outputs/'\nmodel_4_outputs_path = './model_4_outputs/'\nmodel_5_outputs_path = './model_5_outputs/'\nmodel_6_outputs_path = './model_6_outputs/'\n\nprint(rgb_test_images_path)\nprint(psuedo_color_test_images_path)\nprint(model_1_outputs_path)\nprint(model_2_outputs_path)\nprint(model_3_outputs_path)\nprint(model_4_outputs_path)\nprint(model_5_outputs_path)\nprint(model_6_outputs_path)\nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:21:57.722814Z","iopub.execute_input":"2021-08-15T18:21:57.723073Z","iopub.status.idle":"2021-08-15T18:22:03.028665Z","shell.execute_reply.started":"2021-08-15T18:21:57.723044Z","shell.execute_reply":"2021-08-15T18:22:03.027775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_height_width = {}\n\nimage_four_class_classification_model_1, image_four_class_classification_model_2, image_four_class_classification_model_3 = {}, {}, {}\nimage_four_class_classification_model_4, image_four_class_classification_model_5 = {}, {}\n\nimage_four_class_classification_model_6, image_four_class_classification_model_7, image_four_class_classification_model_8 = {}, {}, {}\nimage_four_class_classification_model_9, image_four_class_classification_model_10 = {}, {}\nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:03.030878Z","iopub.execute_input":"2021-08-15T18:22:03.031246Z","iopub.status.idle":"2021-08-15T18:22:03.040383Z","shell.execute_reply.started":"2021-08-15T18:22:03.031202Z","shell.execute_reply":"2021-08-15T18:22:03.039513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\n\nimport os\nimport gc\nimport cv2\nimport glob\nimport efficientnet.tfkeras as efn\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom tqdm.contrib.concurrent import process_map, thread_map\nimport copy\n\n\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\nfrom skimage.filters import unsharp_mask\nfrom skimage import exposure\n\nimport albumentations as albu\n\nimport tensorflow_hub as tf_hub\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:03.042163Z","iopub.execute_input":"2021-08-15T18:22:03.042541Z","iopub.status.idle":"2021-08-15T18:22:10.152719Z","shell.execute_reply.started":"2021-08-15T18:22:03.042505Z","shell.execute_reply":"2021-08-15T18:22:10.151863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"def get_detection_data(input_size=(608, 608), original_size=None, model_outputs=None, class_names=None,\n                       display_num=False):\n    \"\"\"\n    This function takes the output of the YOLOv4 model and deciphers the detections returning a\n    dataframe with the following values: x_min, y_min, x_max, y_max, class_name, confidence_score, bounding_box_width,\n    bounding_box_height\n    :param input_size: input size of the image to the YOLO model\n    :param original_size: tuple containing the size of the actual image\n    :param model_outputs: outputs from inference_model\n    :param class_names: list of object class names\n    :param display_num: Boolean variable to display the number of boxes\n    :return: Pandas dataframe with all the values\n    \"\"\"\n\n    num_bboxes = model_outputs[-1][0]\n    boxes, scores, classes = [output[0][:num_bboxes] for output in model_outputs[:-1]]\n\n    h, w = input_size[:2]\n    df = pd.DataFrame(boxes, columns=['x1', 'y1', 'x2', 'y2'])\n    df.insert(0, 'image_width', original_size[1])\n    df.insert(1, 'image_height', original_size[0])\n    df[['x1', 'x2']] = (df[['x1', 'x2']] * w).astype('int64')\n    df[['y1', 'y2']] = (df[['y1', 'y2']] * h).astype('int64')\n    df['class_name'] = np.array(class_names)[classes.astype('int64')]\n    df['score'] = scores\n    df[['x1', 'x2']] = ((df[['x1', 'x2']].divide(input_size[1])).\n                        multiply(df['image_width'], axis='index')).astype('int64')\n    df[['y1', 'y2']] = ((df[['y1', 'y2']].divide(input_size[0])).\n                        multiply(df['image_height'], axis='index')).astype('int64')\n    df['bounding_box_width'] = df['x2'] - df['x1']\n    df['bounding_box_height'] = df['y2'] - df['y1']\n\n    if display_num:\n        print(f'# of bboxes: {num_bboxes}')\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:10.154068Z","iopub.execute_input":"2021-08-15T18:22:10.154440Z","iopub.status.idle":"2021-08-15T18:22:10.167150Z","shell.execute_reply.started":"2021-08-15T18:22:10.154402Z","shell.execute_reply":"2021-08-15T18:22:10.166364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The submisison requires xmin, ymin, xmax, ymax format. \n# YOLOv5 returns x_center, y_center, width, height\ndef correct_bbox_format(bboxes, H, W):\n    correct_bboxes = []\n    \n    for b in bboxes:\n        xc, yc = int(np.round(b[0] * W)), int(np.round(b[1] * H))\n        w, h = int(np.round(b[2] * W)), int(np.round(b[3] * H))\n\n        xmin = xc - int(np.round(w/2))\n        xmax = xc + int(np.round(w/2))\n        ymin = yc - int(np.round(h/2))\n        ymax = yc + int(np.round(h/2))\n        \n        correct_bboxes.append([xmin, ymin, xmax, ymax])\n        \n    return correct_bboxes\n\n# Read the txt file generated by YOLOv5 during inference and extract \n# confidence and bounding box coordinates.\ndef get_conf_bboxes(file_path):\n    confidence = []\n    bboxes = []\n    with open(file_path, 'r') as file:\n        for line in file:\n            #print(line)\n            preds = line.strip('\\n').split(' ')\n            #print(preds)\n            preds = list(map(float, preds))\n            #print(preds)\n            confidence.append(preds[-1])\n            #print(confidence)\n            bboxes.append(preds[1:-1])\n            #print(bboxes)\n    return confidence, bboxes","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:10.168379Z","iopub.execute_input":"2021-08-15T18:22:10.168898Z","iopub.status.idle":"2021-08-15T18:22:10.179514Z","shell.execute_reply.started":"2021-08-15T18:22:10.168862Z","shell.execute_reply":"2021-08-15T18:22:10.178674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_bounding_box_size(image_name,annotation, percentage):\n    \"\"\"\n    Reduces the bounding box size by a fixed percentage\n    \"\"\"\n    box_width = annotation[4] - annotation[2]\n    box_height = annotation[5] - annotation[3]\n    \n    box_width_delta = ((percentage/2)/100)*box_width\n    box_height_delta = ((percentage/2)/100)*box_height\n    \n    annotation[2], annotation[4] = int(annotation[2] - box_width_delta), int(annotation[4] + box_width_delta)\n    annotation[3], annotation[5] = int(annotation[3] - box_height_delta), int(annotation[5] + box_height_delta)\n    \n    H, W = image_height_width[image_name][0],image_height_width[image_name][1]\n    \n    if annotation[2] < 0:\n        annotation[2] = 0\n    if annotation[3] < 0:\n        annotation[3] = 0\n    if annotation[4] > W:\n        annotation[4] = W\n    if annotation[5] > H:\n        annotation[5] = H\n    \n    return annotation","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:10.180673Z","iopub.execute_input":"2021-08-15T18:22:10.181106Z","iopub.status.idle":"2021-08-15T18:22:10.191723Z","shell.execute_reply.started":"2021-08-15T18:22:10.181073Z","shell.execute_reply":"2021-08-15T18:22:10.190935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bounding_box_sorter(image_name,prediction_string):\n    prediction_string_parts = prediction_string.split(' ')\n    all_predictions = []\n    for i in range(0,len(prediction_string_parts), 6):\n        temp = prediction_string_parts[i:i+6]\n        #print(temp)\n        temp[1], temp[2], temp[3], temp[4], temp[5] = float(temp[1]), int(temp[2]), int(temp[3]), int(temp[4]), int(temp[5])\n        #temp = reduce_bounding_box_size(image_name,temp, 1)\n        all_predictions.append(temp)\n    \n    #print(all_predictions)\n    all_predictions.sort(key = lambda x: x[1])\n    #print(all_predictions)\n    all_predictions = all_predictions[::-1]\n    #print(all_predictions)\n    \n    prediction_string = ''\n    for bbox in all_predictions:\n        prediction_string += bbox[0] +' ' + str(bbox[1]) + ' ' + str(bbox[2]) +' ' + str(bbox[3]) +' ' + str(bbox[4]) +' ' + str(bbox[5]) + ' '\n    \n    return prediction_string[:-1]","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:10.194185Z","iopub.execute_input":"2021-08-15T18:22:10.194538Z","iopub.status.idle":"2021-08-15T18:22:10.203703Z","shell.execute_reply.started":"2021-08-15T18:22:10.194503Z","shell.execute_reply":"2021-08-15T18:22:10.202870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def confidence_score_calibrator(prediction_string, class_confidence):\n    prediction_string_parts = prediction_string.split(' ')\n    all_predictions = []\n    for i in range(0,len(prediction_string_parts), 6):\n        temp = prediction_string_parts[i:i+6]\n        #print(temp)\n        temp[1], temp[2], temp[3], temp[4], temp[5] = float(temp[1]), int(temp[2]), int(temp[3]), int(temp[4]), int(temp[5]) \n        all_predictions.append(temp)\n    \n    none_prediction = all_predictions[-1]\n    all_predictions = all_predictions[:-1]\n    \n    #print(class_confidence)\n    class_confidence_value = class_confidence[np.argmax(class_confidence)]\n    #print(class_confidence_value)\n    for value in all_predictions:\n        value[1] += value[1]-(value[1]*pow(class_confidence_value, 0.1)*pow(none_prediction[1], 0.1))\n        \n    prediction_string = ''\n    for bbox in all_predictions:\n        prediction_string += bbox[0] +' ' + str(bbox[1]) + ' ' + str(bbox[2]) +' ' + str(bbox[3]) +' ' + str(bbox[4]) +' ' + str(bbox[5]) + ' '\n        \n    prediction_srting = bounding_box_sorter(prediction_string[:-1])\n    \n    prediction_string += none_prediction[0] +' ' + str(none_prediction[1]) + ' ' + str(none_prediction[2]) +' ' + str(none_prediction[3]) +' ' + str(none_prediction[4]) +' ' + str(none_prediction[5]) + ' '\n    \n    return prediction_string[:-1]","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:10.205810Z","iopub.execute_input":"2021-08-15T18:22:10.206250Z","iopub.status.idle":"2021-08-15T18:22:10.217248Z","shell.execute_reply.started":"2021-08-15T18:22:10.206215Z","shell.execute_reply":"2021-08-15T18:22:10.216148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def keep_n_top_loaclisations(prediction_string, n):\n    prediction_string_parts = prediction_string.split(' ')\n    total_parts = n*6\n    \n    shorter_prediction_string = ''\n    for i in range(0,total_parts):\n        shorter_prediction_string += prediction_string_parts[i] + ' '\n    \n    shorter_prediction_string += prediction_string_parts[-6] + ' ' + prediction_string_parts[-5] + ' ' + prediction_string_parts[-4] + ' ' + prediction_string_parts[-3] + ' ' + prediction_string_parts[-2] + ' ' + prediction_string_parts[-1]\n    \n    return shorter_prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:10.218713Z","iopub.execute_input":"2021-08-15T18:22:10.219219Z","iopub.status.idle":"2021-08-15T18:22:10.229047Z","shell.execute_reply.started":"2021-08-15T18:22:10.219182Z","shell.execute_reply":"2021-08-15T18:22:10.228116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def denormalise(bboxes, H, W):\n    \"\"\"\n    This function converts the normalised bounding boxes in pascal format to denormalised bounding boxes\n    :param bboxes:\n    :param H:\n    :param W:\n    :return:\n    \"\"\"\n    for box in bboxes:\n        box[0], box[1], box[2], box[3] = int(box[0]*W), int(box[1]*H), int(box[2]*W), int(box[3]*H)\n\n    return bboxes","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:10.230380Z","iopub.execute_input":"2021-08-15T18:22:10.230771Z","iopub.status.idle":"2021-08-15T18:22:10.238306Z","shell.execute_reply.started":"2021-08-15T18:22:10.230735Z","shell.execute_reply":"2021-08-15T18:22:10.237561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bb_intersection_over_union(A, B) -> float:\n    xA = max(A[0], B[0])\n    yA = max(A[1], B[1])\n    xB = min(A[2], B[2])\n    yB = min(A[3], B[3])\n\n    # compute the area of intersection rectangle\n    interArea = max(0, xB - xA) * max(0, yB - yA)\n\n    if interArea == 0:\n        return 0.0\n\n    # compute the area of both the prediction and ground-truth rectangles\n    boxAArea = (A[2] - A[0]) * (A[3] - A[1])\n    boxBArea = (B[2] - B[0]) * (B[3] - B[1])\n\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n    return iou","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:10.239588Z","iopub.execute_input":"2021-08-15T18:22:10.239961Z","iopub.status.idle":"2021-08-15T18:22:10.250847Z","shell.execute_reply.started":"2021-08-15T18:22:10.239926Z","shell.execute_reply":"2021-08-15T18:22:10.250041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prefilter_boxes(boxes, scores, labels, weights, thr):\n    # Create dict with boxes stored by its label\n    new_boxes = dict()\n\n    for t in range(len(boxes)):\n\n        if len(boxes[t]) != len(scores[t]):\n            print('Error. Length of boxes arrays not equal to length of scores array: {} != {}'.format(len(boxes[t]), len(scores[t])))\n            exit()\n\n        if len(boxes[t]) != len(labels[t]):\n            print('Error. Length of boxes arrays not equal to length of labels array: {} != {}'.format(len(boxes[t]), len(labels[t])))\n            exit()\n\n        for j in range(len(boxes[t])):\n            score = scores[t][j]\n            if score < thr:\n                continue\n            label = int(labels[t][j])\n            box_part = boxes[t][j]\n            x1 = float(box_part[0])\n            y1 = float(box_part[1])\n            x2 = float(box_part[2])\n            y2 = float(box_part[3])\n\n            # Box data checks\n            if x2 < x1:\n                warnings.warn('X2 < X1 value in box. Swap them.')\n                x1, x2 = x2, x1\n            if y2 < y1:\n                warnings.warn('Y2 < Y1 value in box. Swap them.')\n                y1, y2 = y2, y1\n            if x1 < 0:\n                warnings.warn('X1 < 0 in box. Set it to 0.')\n                x1 = 0\n            if x1 > 1:\n                warnings.warn('X1 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n                x1 = 1\n            if x2 < 0:\n                warnings.warn('X2 < 0 in box. Set it to 0.')\n                x2 = 0\n            if x2 > 1:\n                warnings.warn('X2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n                x2 = 1\n            if y1 < 0:\n                warnings.warn('Y1 < 0 in box. Set it to 0.')\n                y1 = 0\n            if y1 > 1:\n                warnings.warn('Y1 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n                y1 = 1\n            if y2 < 0:\n                warnings.warn('Y2 < 0 in box. Set it to 0.')\n                y2 = 0\n            if y2 > 1:\n                warnings.warn('Y2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n                y2 = 1\n            if (x2 - x1) * (y2 - y1) == 0.0:\n                warnings.warn(\"Zero area box skipped: {}.\".format(box_part))\n                continue\n\n            # [label, score, weight, model index, x1, y1, x2, y2]\n            b = [int(label), float(score) * weights[t], weights[t], t, x1, y1, x2, y2]\n            if label not in new_boxes:\n                new_boxes[label] = []\n            new_boxes[label].append(b)\n\n    # Sort each list in dict by score and transform it to numpy array\n    for k in new_boxes:\n        current_boxes = np.array(new_boxes[k])\n        new_boxes[k] = current_boxes[current_boxes[:, 1].argsort()[::-1]]\n\n    return new_boxes","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:10.252112Z","iopub.execute_input":"2021-08-15T18:22:10.252584Z","iopub.status.idle":"2021-08-15T18:22:10.269432Z","shell.execute_reply.started":"2021-08-15T18:22:10.252546Z","shell.execute_reply":"2021-08-15T18:22:10.268455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_weighted_box(boxes, conf_type='avg'):\n    \"\"\"\n    Create weighted box for set of boxes\n    :param boxes: set of boxes to fuse\n    :param conf_type: type of confidence one of 'avg' or 'max'\n    :return: weighted box (label, score, weight, x1, y1, x2, y2)\n    \"\"\"\n\n    box = np.zeros(8, dtype=np.float32)\n    conf = 0\n    conf_list = []\n    w = 0\n    for b in boxes:\n        box[4:] += (b[1] * b[4:])\n        conf += b[1]\n        conf_list.append(b[1])\n        w += b[2]\n    box[0] = boxes[0][0]\n    if conf_type == 'avg':\n        box[1] = conf / len(boxes)\n    elif conf_type == 'max':\n        box[1] = np.array(conf_list).max()\n    elif conf_type in ['box_and_model_avg', 'absent_model_aware_avg']:\n        box[1] = conf / len(boxes)\n    box[2] = w\n    box[3] = -1 # model index field is retained for consistensy but is not used.\n    box[4:] /= conf\n    return box","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:10.270697Z","iopub.execute_input":"2021-08-15T18:22:10.271105Z","iopub.status.idle":"2021-08-15T18:22:10.281452Z","shell.execute_reply.started":"2021-08-15T18:22:10.271071Z","shell.execute_reply":"2021-08-15T18:22:10.280654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_matching_box(boxes_list, new_box, match_iou):\n    best_iou = match_iou\n    best_index = -1\n    for i in range(len(boxes_list)):\n        box = boxes_list[i]\n        if box[0] != new_box[0]:\n            continue\n        iou = bb_intersection_over_union(box[4:], new_box[4:])\n        if iou > best_iou:\n            best_index = i\n            best_iou = iou\n\n    return best_index, best_iou","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:10.282563Z","iopub.execute_input":"2021-08-15T18:22:10.283019Z","iopub.status.idle":"2021-08-15T18:22:10.291214Z","shell.execute_reply.started":"2021-08-15T18:22:10.282984Z","shell.execute_reply":"2021-08-15T18:22:10.290410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=None, iou_thr=0.55, skip_box_thr=0.0, conf_type='box_and_model_avg', allows_overflow=False):\n    '''\n    :param boxes_list: list of boxes predictions from each model, each box is 4 numbers.\n    It has 3 dimensions (models_number, model_preds, 4)\n    Order of boxes: x1, y1, x2, y2. We expect float normalized coordinates [0; 1]\n    :param scores_list: list of scores for each model\n    :param labels_list: list of labels for each model\n    :param weights: list of weights for each model. Default: None, which means weight == 1 for each model\n    :param iou_thr: IoU value for boxes to be a match\n    :param skip_box_thr: exclude boxes with score lower than this variable\n    :param conf_type: how to calculate confidence in weighted boxes. 'avg': average value, 'max': maximum value, 'box_and_model_avg': box and model wise hybrid weighted average, 'absent_model_aware_avg': weighted average that takes into account the absent model.\n    :param allows_overflow: false if we want confidence score not exceed 1.0\n    :return: boxes: boxes coordinates (Order of boxes: x1, y1, x2, y2).\n    :return: scores: confidence scores\n    :return: labels: boxes labels\n    '''\n\n    if weights is None:\n        weights = np.ones(len(boxes_list))\n    if len(weights) != len(boxes_list):\n        print('Warning: incorrect number of weights {}. Must be: {}. Set weights equal to 1.'.format(len(weights), len(boxes_list)))\n        weights = np.ones(len(boxes_list))\n    weights = np.array(weights)\n\n    if conf_type not in ['avg', 'max', 'box_and_model_avg', 'absent_model_aware_avg']:\n        print('Unknown conf_type: {}. Must be \"avg\", \"max\" or \"box_and_model_avg\", or \"absent_model_aware_avg\"'.format(conf_type))\n        exit()\n\n    filtered_boxes = prefilter_boxes(boxes_list, scores_list, labels_list, weights, skip_box_thr)\n    if len(filtered_boxes) == 0:\n        return np.zeros((0, 4)), np.zeros((0,)), np.zeros((0,))\n\n    overall_boxes = []\n    for label in filtered_boxes:\n        boxes = filtered_boxes[label]\n        new_boxes = []\n        weighted_boxes = []\n        # Clusterize boxes\n        for j in range(0, len(boxes)):\n            index, best_iou = find_matching_box(weighted_boxes, boxes[j], iou_thr)\n            if index != -1:\n                new_boxes[index].append(boxes[j])\n                weighted_boxes[index] = get_weighted_box(new_boxes[index], conf_type)\n            else:\n                new_boxes.append([boxes[j].copy()])\n                weighted_boxes.append(boxes[j].copy())\n        # Rescale confidence based on number of models and boxes\n        for i in range(len(new_boxes)):\n            clustered_boxes = np.array(new_boxes[i])\n            if conf_type == 'box_and_model_avg':\n                # weighted average for boxes\n                weighted_boxes[i][1] = weighted_boxes[i][1] * len(clustered_boxes) / weighted_boxes[i][2]\n                # identify unique model index by model index column\n                _, idx = np.unique(clustered_boxes[:, 3], return_index=True)\n                # rescale by unique model weights\n                weighted_boxes[i][1] = weighted_boxes[i][1] *  clustered_boxes[idx, 2].sum() / weights.sum()\n\n            elif conf_type == 'absent_model_aware_avg':\n                # get unique model index in the cluster\n                models = np.unique(clustered_boxes[:, 3]).astype(int)\n                # create a mask to get unused model weights\n                mask = np.ones(len(weights), dtype=bool)\n                mask[models] = False\n                # absent model aware weighted average\n                weighted_boxes[i][1] = weighted_boxes[i][1] * len(clustered_boxes) / (weighted_boxes[i][2] + weights[mask].sum())\n            elif not allows_overflow:\n                weighted_boxes[i][1] = weighted_boxes[i][1] * min(weights.sum(), len(clustered_boxes)) / weights.sum()\n            else:\n                weighted_boxes[i][1] = weighted_boxes[i][1] * len(clustered_boxes) / weights.sum()\n        overall_boxes.append(np.array(weighted_boxes))\n    overall_boxes = np.concatenate(overall_boxes, axis=0)\n    overall_boxes = overall_boxes[overall_boxes[:, 1].argsort()[::-1]]\n    boxes = overall_boxes[:, 4:]\n    scores = overall_boxes[:, 1]\n    labels = overall_boxes[:, 0]\n    return boxes, scores, labels","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:10.292389Z","iopub.execute_input":"2021-08-15T18:22:10.292756Z","iopub.status.idle":"2021-08-15T18:22:10.310092Z","shell.execute_reply.started":"2021-08-15T18:22:10.292721Z","shell.execute_reply":"2021-08-15T18:22:10.309314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_extraction_and_saving(path):\n    global image_height_width\n    my_dicom = pydicom.dcmread(path)\n\n    image_data = apply_voi_lut(my_dicom.pixel_array, my_dicom)\n\n    if my_dicom.PhotometricInterpretation == 'MONOCHROME1':\n        image_data = np.amax(image_data) - image_data\n\n    image_data = image_data - np.min(image_data)\n    image_data = image_data / np.max(image_data)\n    image_data = (image_data*255).astype(np.uint8)\n\n\n    my_shape = np.shape(image_data)\n    file_parts = path.split('/')\n    image_height_width[file_parts[-1].split('.')[0]+'_image'] = my_shape\n    cv2.imwrite(os.path.join(rgb_test_images_path, file_parts[-3]+'_'+file_parts[-2]+'_'+file_parts[-1].split('.')[0]+'.jpeg'), np.stack((cv2.resize(image_data, (1280,1280)),)*3, axis=-1))\n    cv2.imwrite(os.path.join(psuedo_color_test_images_path, file_parts[-3]+'_'+file_parts[-2]+'_'+file_parts[-1].split('.')[0]+'.jpeg'), cv2.applyColorMap(np.stack((cv2.resize(image_data, (1280,1280)),)*3, axis=-1), cv2.COLORMAP_JET))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:10.311360Z","iopub.execute_input":"2021-08-15T18:22:10.311829Z","iopub.status.idle":"2021-08-15T18:22:10.323122Z","shell.execute_reply.started":"2021-08-15T18:22:10.311797Z","shell.execute_reply":"2021-08-15T18:22:10.322390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extracting Images","metadata":{}},{"cell_type":"code","source":"all_files = glob.glob('../input/siim-covid19-detection/test' + '/**/*.dcm', recursive=True)\n\nif len(pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')) == 2477:\n    all_files = all_files[:10]","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:10.324191Z","iopub.execute_input":"2021-08-15T18:22:10.324488Z","iopub.status.idle":"2021-08-15T18:22:17.338761Z","shell.execute_reply.started":"2021-08-15T18:22:10.324458Z","shell.execute_reply":"2021-08-15T18:22:17.337859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = thread_map(image_extraction_and_saving, all_files, max_workers=8, chunksize=1)\nprint('All files extracted')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:17.339947Z","iopub.execute_input":"2021-08-15T18:22:17.340301Z","iopub.status.idle":"2021-08-15T18:22:23.150952Z","shell.execute_reply.started":"2021-08-15T18:22:17.340250Z","shell.execute_reply":"2021-08-15T18:22:23.150120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:23.152333Z","iopub.execute_input":"2021-08-15T18:22:23.152881Z","iopub.status.idle":"2021-08-15T18:22:23.312845Z","shell.execute_reply.started":"2021-08-15T18:22:23.152842Z","shell.execute_reply":"2021-08-15T18:22:23.311638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_files = os.listdir(rgb_test_images_path)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:23.314428Z","iopub.execute_input":"2021-08-15T18:22:23.314894Z","iopub.status.idle":"2021-08-15T18:22:23.321082Z","shell.execute_reply.started":"2021-08-15T18:22:23.314853Z","shell.execute_reply":"2021-08-15T18:22:23.320196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Class for 4-Class Classification Models","metadata":{}},{"cell_type":"code","source":"MODEL_ARCH = 'efficientnetv2-l-21k-ft1k'\n# Get the TensorFlow Hub model URL\nhub_type = 'feature_vector' # ['classification', 'feature_vector']\nMODEL_ARCH_PATH = f'../input/efficientnetv2-tfhub-weight-files/tfhub_models/{MODEL_ARCH}/{hub_type}'\n\n# Custom wrapper class to load the right pretrained weights explicitly from the local directory\nclass KerasLayerWrapper(tf_hub.KerasLayer):\n    def __init__(self, handle, **kwargs):\n        handle = tf_hub.KerasLayer(tf_hub.load(MODEL_ARCH_PATH))\n        super().__init__(handle, **kwargs)\n        \nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:23.322378Z","iopub.execute_input":"2021-08-15T18:22:23.322847Z","iopub.status.idle":"2021-08-15T18:22:23.332206Z","shell.execute_reply.started":"2021-08-15T18:22:23.322810Z","shell.execute_reply":"2021-08-15T18:22:23.331349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Study Level (4 Class) Classification**","metadata":{}},{"cell_type":"code","source":"my_submission_file_1_study = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\nmy_submission_file_2_study = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\nmy_submission_file_3_study = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\nmy_submission_file_4_study = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\nmy_submission_file_5_study = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\n\nimage_size_1 = (768, 768, 3)\nimage_size_2 = (768, 768, 3)\nimage_size_3 = (768, 768, 3)\nimage_size_4 = (768, 768, 3)\nimage_size_5 = (768, 768, 3)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:23.337677Z","iopub.execute_input":"2021-08-15T18:22:23.338187Z","iopub.status.idle":"2021-08-15T18:22:23.361914Z","shell.execute_reply.started":"2021-08-15T18:22:23.338151Z","shell.execute_reply":"2021-08-15T18:22:23.361162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpus = tf.config.experimental.list_physical_devices('GPU')\ntf.config.experimental.set_virtual_device_configuration(gpus[0],\n                 [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=13000)])\nstrategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')\n\nmy_model_1 = tf.keras.models.load_model('../input/d/asadmkhan/siim-classification-models/model0_ver15.h5')\nmy_model_1 = tf.keras.Model(inputs=my_model_1.input, outputs=my_model_1.get_layer('Classification_Head').output)\nprint('Model 1 Loaded')\n\nstudy_level = my_submission_file_1_study[my_submission_file_1_study['id'].str.contains('_study')]\nprint(study_level.head())\nprint(len(study_level))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:23.364608Z","iopub.execute_input":"2021-08-15T18:22:23.364915Z","iopub.status.idle":"2021-08-15T18:22:46.436755Z","shell.execute_reply.started":"2021-08-15T18:22:23.364887Z","shell.execute_reply":"2021-08-15T18:22:46.432979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for file in tqdm(range(len(study_level))):\n    my_row = study_level.loc[file].values.tolist()\n    study_name = my_row[0].split('_')\n\n    complete_name = [x for x in all_files if study_name[0] in x]\n    \n    my_predictions_study = np.zeros((1,4))\n    for images in complete_name:\n        #print(images)\n        \n        image_data = cv2.imread(os.path.join(rgb_test_images_path, images), -1)\n        if image_data is not None:\n            my_image_1 = cv2.resize(image_data, image_size_1[:2])/255.0\n            my_image_expanded_1 = np.zeros((3,*image_size_1))\n            my_image_expanded_1[0,...] = my_image_1\n            my_image_expanded_1[1,...] = np.fliplr(my_image_1)\n            my_image_expanded_1[2,...] = np.roll(my_image_1, 30, axis=1)\n            \n            my_predictions_1 = my_model_1.predict(my_image_expanded_1)\n            \n            #try:\n            #    index = np.argmax(my_predictions_1[:,statistics.mode(np.argmax(my_predictions_1, axis=0))],axis=0)\n            #    my_predictions_1 = np.reshape(my_predictions_1[index,:], (1,4))\n            #except:\n            my_predictions_1 = np.reshape(np.mean(my_predictions_1, axis=0), (1,4))\n            \n            \n            del my_image_1\n            del my_image_expanded_1\n            \n            my_predictions_study = np.concatenate((my_predictions_study, my_predictions_1))\n            \n            #my_prediction_image = np.mean(np.vstack((my_predictions_1)), axis=0)\n            image_four_class_classification_model_1[images.split('_')[-1].split('.')[0]+'_image'] = my_predictions_1\n            \n            \n            \n    #print(my_predictions_study)\n    if len(my_predictions_study) > 1:\n        my_predictions_study = list(my_predictions_study[1,:])\n\n\n        prediction_string = 'negative' + ' ' + str(my_predictions_study[0]) + ' 0 0 1 1 ' + 'typical' + ' ' + str(my_predictions_study[1]) + ' 0 0 1 1 ' + 'indeterminate' + ' ' + str(my_predictions_study[2]) + ' 0 0 1 1 ' + 'atypical' + ' ' + str(my_predictions_study[3]) + ' 0 0 1 1' \n\n        #print(prediction_string)\n\n        search_name = study_name[0] + '_study'\n        my_submission_file_1_study['PredictionString'][my_submission_file_1_study.id==search_name] = prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.437643Z","iopub.status.idle":"2021-08-15T18:22:46.438040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission_file_1_study","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.439316Z","iopub.status.idle":"2021-08-15T18:22:46.439860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del my_model_1\ngc.collect()\nmy_submission_file_1_study","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.441011Z","iopub.status.idle":"2021-08-15T18:22:46.441746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_model_2 = tf.keras.models.load_model('../input/d/asadmkhan/siim-classification-models/model1_ver15.h5')\nmy_model_2 = tf.keras.Model(inputs=my_model_2.input, outputs=my_model_2.get_layer('Classification_Head').output)\n\nprint('Model 2 Loaded')\n\nstudy_level = my_submission_file_2_study[my_submission_file_2_study['id'].str.contains('_study')]\nprint(study_level.head())\nprint(len(study_level))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.443370Z","iopub.status.idle":"2021-08-15T18:22:46.444037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for file in tqdm(range(len(study_level))):\n    my_row = study_level.loc[file].values.tolist()\n    study_name = my_row[0].split('_')\n\n    complete_name = [x for x in all_files if study_name[0] in x]\n    \n    my_predictions_study = np.zeros((1,4))\n    for images in complete_name:\n        #print(images)\n        \n        image_data = cv2.imread(os.path.join(rgb_test_images_path, images), -1)\n        if image_data is not None:\n            my_image_2 = cv2.resize(image_data, image_size_2[:2])/255.0\n            my_image_expanded_2 = np.zeros((3,*image_size_2))\n            my_image_expanded_2[0,...] = my_image_2\n            my_image_expanded_2[1,...] = np.fliplr(my_image_2)\n            my_image_expanded_2[2,...] = np.roll(my_image_2, 30, axis=1)\n            \n            my_predictions_2 = my_model_2.predict(my_image_expanded_2)\n            \n            #try:\n            #    index = np.argmax(my_predictions_2[:,statistics.mode(np.argmax(my_predictions_2, axis=0))],axis=0)\n            #    my_predictions_2 = np.reshape(my_predictions_2[index,:], (1,4))\n            #except:\n            my_predictions_2 = np.reshape(np.mean(my_predictions_2, axis=0), (1,4))\n            \n            \n            del my_image_2\n            del my_image_expanded_2\n            \n            my_predictions_study = np.concatenate((my_predictions_study, my_predictions_2))\n            \n            #my_prediction_image = np.mean(np.vstack((my_predictions_1)), axis=0)\n            image_four_class_classification_model_2[images.split('_')[-1].split('.')[0]+'_image'] = my_predictions_2\n            \n            \n    #print(my_predictions_study)\n    if len(my_predictions_study) > 1:\n        my_predictions_study = list(my_predictions_study[1,:])\n\n\n        prediction_string = 'negative' + ' ' + str(my_predictions_study[0]) + ' 0 0 1 1 ' + 'typical' + ' ' + str(my_predictions_study[1]) + ' 0 0 1 1 ' + 'indeterminate' + ' ' + str(my_predictions_study[2]) + ' 0 0 1 1 ' + 'atypical' + ' ' + str(my_predictions_study[3]) + ' 0 0 1 1' \n\n        #print(prediction_string)\n\n        search_name = study_name[0] + '_study'\n        my_submission_file_2_study['PredictionString'][my_submission_file_2_study.id==search_name] = prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.445354Z","iopub.status.idle":"2021-08-15T18:22:46.445957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del my_model_2\ngc.collect()\n\nmy_submission_file_2_study","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.447179Z","iopub.status.idle":"2021-08-15T18:22:46.447818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_model_3 = tf.keras.models.load_model('../input/d/asadmkhan/siim-classification-models/model2_ver15.h5')\nmy_model_3 = tf.keras.Model(inputs=my_model_3.input, outputs=my_model_3.get_layer('Classification_Head').output)\nprint('Model 3 Loaded')\n\nstudy_level = my_submission_file_3_study[my_submission_file_3_study['id'].str.contains('_study')]\nprint(study_level.head())\nprint(len(study_level))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.448984Z","iopub.status.idle":"2021-08-15T18:22:46.449612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for file in tqdm(range(len(study_level))):\n    my_row = study_level.loc[file].values.tolist()\n    study_name = my_row[0].split('_')\n\n    complete_name = [x for x in all_files if study_name[0] in x]\n    \n    my_predictions_study = np.zeros((1,4))\n    for images in complete_name:\n        #print(images)\n        \n        image_data = cv2.imread(os.path.join(rgb_test_images_path, images), -1)\n        if image_data is not None:\n            my_image_3 = cv2.resize(image_data, image_size_3[:2])/255.0\n            my_image_expanded_3 = np.zeros((3,*image_size_3))\n            my_image_expanded_3[0,...] = my_image_3\n            my_image_expanded_3[1,...] = np.fliplr(my_image_3)\n            my_image_expanded_3[2,...] = np.roll(my_image_3, 30, axis=1)\n            \n            my_predictions_3 = my_model_3.predict(my_image_expanded_3)\n            \n            #try:\n            #    index = np.argmax(my_predictions_3[:,statistics.mode(np.argmax(my_predictions_3, axis=0))],axis=0)\n            #    my_predictions_3 = np.reshape(my_predictions_3[index,:], (1,4))\n            #except:\n            my_predictions_3 = np.reshape(np.mean(my_predictions_3, axis=0), (1,4))\n            \n            del my_image_3\n            del my_image_expanded_3\n            \n            my_predictions_study = np.concatenate((my_predictions_study, my_predictions_3))\n            \n            #my_prediction_image = np.mean(np.vstack((my_predictions_1)), axis=0)\n            image_four_class_classification_model_3[images.split('_')[-1].split('.')[0]+'_image'] = my_predictions_3\n            \n            \n    #print(my_predictions_study)\n    if len(my_predictions_study) > 1:\n        my_predictions_study = list(my_predictions_study[1,:])\n\n\n        prediction_string = 'negative' + ' ' + str(my_predictions_study[0]) + ' 0 0 1 1 ' + 'typical' + ' ' + str(my_predictions_study[1]) + ' 0 0 1 1 ' + 'indeterminate' + ' ' + str(my_predictions_study[2]) + ' 0 0 1 1 ' + 'atypical' + ' ' + str(my_predictions_study[3]) + ' 0 0 1 1' \n\n        #print(prediction_string)\n\n        search_name = study_name[0] + '_study'\n        my_submission_file_3_study['PredictionString'][my_submission_file_3_study.id==search_name] = prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.450872Z","iopub.status.idle":"2021-08-15T18:22:46.451492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del my_model_3\ngc.collect()\n\nmy_submission_file_3_study","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.452645Z","iopub.status.idle":"2021-08-15T18:22:46.453304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_model_4 = tf.keras.models.load_model('../input/d/asadmkhan/siim-classification-models/model3_ver15.h5')\nmy_model_4 = tf.keras.Model(inputs=my_model_4.input, outputs=my_model_4.get_layer('Classification_Head').output)\nprint('Model 4 Loaded')\n\nstudy_level = my_submission_file_4_study[my_submission_file_4_study['id'].str.contains('_study')]\nprint(study_level.head())\nprint(len(study_level))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.454520Z","iopub.status.idle":"2021-08-15T18:22:46.455134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for file in tqdm(range(len(study_level))):\n    my_row = study_level.loc[file].values.tolist()\n    study_name = my_row[0].split('_')\n\n    complete_name = [x for x in all_files if study_name[0] in x]\n    \n    my_predictions_study = np.zeros((1,4))\n    for images in complete_name:\n        #print(images)\n        \n        image_data = cv2.imread(os.path.join(rgb_test_images_path, images), -1)\n        if image_data is not None:\n            my_image_4 = cv2.resize(image_data, image_size_4[:2])/255.0\n            my_image_expanded_4 = np.zeros((3,*image_size_4))\n            my_image_expanded_4[0,...] = my_image_4\n            my_image_expanded_4[1,...] = np.fliplr(my_image_4)\n            my_image_expanded_4[2,...] = np.roll(my_image_4, 30, axis=1)\n            \n            my_predictions_4 = my_model_4.predict(my_image_expanded_4)\n            \n            #try:\n            #    index = np.argmax(my_predictions_4[:,statistics.mode(np.argmax(my_predictions_4, axis=0))],axis=0)\n            #    my_predictions_4 = np.reshape(my_predictions_4[index,:], (1,4))\n            #except:\n            my_predictions_4 = np.reshape(np.mean(my_predictions_4, axis=0), (1,4))\n            \n            del my_image_4\n            del my_image_expanded_4\n            \n            my_predictions_study = np.concatenate((my_predictions_study, my_predictions_4))\n            \n            #my_prediction_image = np.mean(np.vstack((my_predictions_1)), axis=0)\n            image_four_class_classification_model_4[images.split('_')[-1].split('.')[0]+'_image'] = my_predictions_4\n            \n            \n    #print(my_predictions_study)\n    if len(my_predictions_study) > 1:\n        my_predictions_study = list(my_predictions_study[1,:])\n\n\n        prediction_string = 'negative' + ' ' + str(my_predictions_study[0]) + ' 0 0 1 1 ' + 'typical' + ' ' + str(my_predictions_study[1]) + ' 0 0 1 1 ' + 'indeterminate' + ' ' + str(my_predictions_study[2]) + ' 0 0 1 1 ' + 'atypical' + ' ' + str(my_predictions_study[3]) + ' 0 0 1 1' \n\n        #print(prediction_string)\n\n        search_name = study_name[0] + '_study'\n        my_submission_file_4_study['PredictionString'][my_submission_file_4_study.id==search_name] = prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.456452Z","iopub.status.idle":"2021-08-15T18:22:46.457041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del my_model_4\ngc.collect()\n\nmy_submission_file_4_study","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.458272Z","iopub.status.idle":"2021-08-15T18:22:46.458922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_model_5 = tf.keras.models.load_model('../input/d/asadmkhan/siim-classification-models/model4_ver15.h5')\nmy_model_5 = tf.keras.Model(inputs=my_model_5.input, outputs=my_model_5.get_layer('Classification_Head').output)\nprint('Model 5 Loaded')\n\nstudy_level = my_submission_file_5_study[my_submission_file_5_study['id'].str.contains('_study')]\nprint(study_level.head())\nprint(len(study_level))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.460116Z","iopub.status.idle":"2021-08-15T18:22:46.460773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for file in tqdm(range(len(study_level))):\n    my_row = study_level.loc[file].values.tolist()\n    study_name = my_row[0].split('_')\n\n    complete_name = [x for x in all_files if study_name[0] in x]\n    \n    my_predictions_study = np.zeros((1,4))\n    for images in complete_name:\n        #print(images)\n        \n        image_data = cv2.imread(os.path.join(rgb_test_images_path, images), -1)\n        if image_data is not None:\n            my_image_5 = cv2.resize(image_data, image_size_5[:2])/255.0\n            my_image_expanded_5 = np.zeros((3,*image_size_5))\n            my_image_expanded_5[0,...] = my_image_5\n            my_image_expanded_5[1,...] = np.fliplr(my_image_5)\n            my_image_expanded_5[2,...] = np.roll(my_image_5, 30, axis=1)\n            \n            my_predictions_5 = my_model_5.predict(my_image_expanded_5)\n            \n            #try:\n            #    index = np.argmax(my_predictions_5[:,statistics.mode(np.argmax(my_predictions_5, axis=0))],axis=0)\n            #    my_predictions_5 = np.reshape(my_predictions_5[index,:], (1,4))\n            #except:\n            my_predictions_5 = np.reshape(np.mean(my_predictions_5, axis=0), (1,4))\n            \n            del my_image_5\n            del my_image_expanded_5\n            \n            my_predictions_study = np.concatenate((my_predictions_study, my_predictions_5))\n            \n            #my_prediction_image = np.mean(np.vstack((my_predictions_1)), axis=0)\n            image_four_class_classification_model_5[images.split('_')[-1].split('.')[0]+'_image'] = my_predictions_5\n            \n            \n    #print(my_predictions_study)\n    if len(my_predictions_study) > 1:\n        my_predictions_study = list(my_predictions_study[1,:])\n\n\n        prediction_string = 'negative' + ' ' + str(my_predictions_study[0]) + ' 0 0 1 1 ' + 'typical' + ' ' + str(my_predictions_study[1]) + ' 0 0 1 1 ' + 'indeterminate' + ' ' + str(my_predictions_study[2]) + ' 0 0 1 1 ' + 'atypical' + ' ' + str(my_predictions_study[3]) + ' 0 0 1 1' \n\n        #print(prediction_string)\n\n        search_name = study_name[0] + '_study'\n        my_submission_file_5_study['PredictionString'][my_submission_file_5_study.id==search_name] = prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.462038Z","iopub.status.idle":"2021-08-15T18:22:46.462684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del my_model_5\ngc.collect()\n\nmy_submission_file_4_study","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.463828Z","iopub.status.idle":"2021-08-15T18:22:46.464465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.465597Z","iopub.status.idle":"2021-08-15T18:22:46.466222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission_file_6_study = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\nmy_submission_file_7_study = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\nmy_submission_file_8_study = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\nmy_submission_file_9_study = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\nmy_submission_file_10_study = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\n\nimage_size_6 = (768, 768, 3)\nimage_size_7 = (768, 768, 3)\nimage_size_8 = (768, 768, 3)\nimage_size_9 = (768, 768, 3)\nimage_size_10 = (768, 768, 3)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.467437Z","iopub.status.idle":"2021-08-15T18:22:46.467989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_model_6 = tf.keras.models.load_model('../input/siim-effnetv2-keras-study-train-tpu-cv0-805/model0_ver16.h5')\nmy_model_6 = tf.keras.Model(inputs=my_model_6.input, outputs=my_model_6.get_layer('Classification_Head').output)\nprint('Model 6 Loaded')\n\nstudy_level = my_submission_file_6_study[my_submission_file_6_study['id'].str.contains('_study')]\nprint(study_level.head())\nprint(len(study_level))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.469061Z","iopub.status.idle":"2021-08-15T18:22:46.469726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for file in tqdm(range(len(study_level))):\n    my_row = study_level.loc[file].values.tolist()\n    study_name = my_row[0].split('_')\n\n    complete_name = [x for x in all_files if study_name[0] in x]\n    \n    my_predictions_study = np.zeros((1,4))\n    for images in complete_name:\n        #print(images)\n        \n        image_data = cv2.imread(os.path.join(rgb_test_images_path, images), -1)\n        if image_data is not None:\n            my_image_6 = cv2.resize(image_data, image_size_6[:2])/255.0\n            my_image_expanded_6 = np.zeros((3,*image_size_6))\n            my_image_expanded_6[0,...] = my_image_6\n            my_image_expanded_6[1,...] = np.fliplr(my_image_6)\n            my_image_expanded_6[2,...] = np.roll(my_image_6, 30, axis=1)\n            \n            my_predictions_6 = my_model_6.predict(my_image_expanded_6)\n            \n            #try:\n            #    index = np.argmax(my_predictions_6[:,statistics.mode(np.argmax(my_predictions_6, axis=0))],axis=0)\n            #    my_predictions_6 = np.reshape(my_predictions_6[index,:], (1,4))\n            #except:\n            my_predictions_6 = np.reshape(np.mean(my_predictions_6, axis=0), (1,4))\n            \n            del my_image_6\n            del my_image_expanded_6\n            \n            my_predictions_study = np.concatenate((my_predictions_study, my_predictions_6))\n            \n            #my_prediction_image = np.mean(np.vstack((my_predictions_1)), axis=0)\n            image_four_class_classification_model_6[images.split('_')[-1].split('.')[0]+'_image'] = my_predictions_6\n            \n            \n    #print(my_predictions_study)\n    if len(my_predictions_study) > 1:\n        my_predictions_study = list(my_predictions_study[1,:])\n\n\n        prediction_string = 'negative' + ' ' + str(my_predictions_study[0]) + ' 0 0 1 1 ' + 'typical' + ' ' + str(my_predictions_study[1]) + ' 0 0 1 1 ' + 'indeterminate' + ' ' + str(my_predictions_study[2]) + ' 0 0 1 1 ' + 'atypical' + ' ' + str(my_predictions_study[3]) + ' 0 0 1 1' \n\n        #print(prediction_string)\n\n        search_name = study_name[0] + '_study'\n        my_submission_file_6_study['PredictionString'][my_submission_file_6_study.id==search_name] = prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.471151Z","iopub.status.idle":"2021-08-15T18:22:46.471768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del my_model_6\ngc.collect()\nmy_submission_file_6_study","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.472867Z","iopub.status.idle":"2021-08-15T18:22:46.473486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_model_7 = tf.keras.models.load_model('../input/siim-effnetv2-keras-study-train-tpu-cv0-805/model1_ver16.h5')\nmy_model_7 = tf.keras.Model(inputs=my_model_7.input, outputs=my_model_7.get_layer('Classification_Head').output)\nprint('Model 7 Loaded')\n\nstudy_level = my_submission_file_7_study[my_submission_file_7_study['id'].str.contains('_study')]\nprint(study_level.head())\nprint(len(study_level))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.474561Z","iopub.status.idle":"2021-08-15T18:22:46.475161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for file in tqdm(range(len(study_level))):\n    my_row = study_level.loc[file].values.tolist()\n    study_name = my_row[0].split('_')\n\n    complete_name = [x for x in all_files if study_name[0] in x]\n    \n    my_predictions_study = np.zeros((1,4))\n    for images in complete_name:\n        #print(images)\n        \n        image_data = cv2.imread(os.path.join(rgb_test_images_path, images), -1)\n        if image_data is not None:\n            my_image_7 = cv2.resize(image_data, image_size_7[:2])/255.0\n            my_image_expanded_7 = np.zeros((3,*image_size_7))\n            my_image_expanded_7[0,...] = my_image_7\n            my_image_expanded_7[1,...] = np.fliplr(my_image_7)\n            my_image_expanded_7[2,...] = np.roll(my_image_7, 30, axis=1)\n            \n            my_predictions_7 = my_model_7.predict(my_image_expanded_7)\n            \n            #try:\n            #    index = np.argmax(my_predictions_7[:,statistics.mode(np.argmax(my_predictions_7, axis=0))],axis=0)\n            #    my_predictions_7 = np.reshape(my_predictions_7[index,:], (1,4))\n            #except:\n            my_predictions_7 = np.reshape(np.mean(my_predictions_7, axis=0), (1,4))\n            \n            \n            del my_image_7\n            del my_image_expanded_7\n            \n            my_predictions_study = np.concatenate((my_predictions_study, my_predictions_7))\n            \n            #my_prediction_image = np.mean(np.vstack((my_predictions_1)), axis=0)\n            image_four_class_classification_model_7[images.split('_')[-1].split('.')[0]+'_image'] = my_predictions_7\n            \n            \n    if len(my_predictions_study) > 1:\n        my_predictions_study = list(my_predictions_study[1,:])\n\n\n        prediction_string = 'negative' + ' ' + str(my_predictions_study[0]) + ' 0 0 1 1 ' + 'typical' + ' ' + str(my_predictions_study[1]) + ' 0 0 1 1 ' + 'indeterminate' + ' ' + str(my_predictions_study[2]) + ' 0 0 1 1 ' + 'atypical' + ' ' + str(my_predictions_study[3]) + ' 0 0 1 1' \n\n        #print(prediction_string)\n\n        search_name = study_name[0] + '_study'\n        my_submission_file_7_study['PredictionString'][my_submission_file_7_study.id==search_name] = prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.476418Z","iopub.status.idle":"2021-08-15T18:22:46.476970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del my_model_7\ngc.collect()\n\nmy_submission_file_7_study","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.478127Z","iopub.status.idle":"2021-08-15T18:22:46.478758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_model_8 = tf.keras.models.load_model('../input/siim-effnetv2-keras-study-train-tpu-cv0-805/model2_ver16.h5')\nmy_model_8 = tf.keras.Model(inputs=my_model_8.input, outputs=my_model_8.get_layer('Classification_Head').output)\nprint('Model 8 Loaded')\n\nstudy_level = my_submission_file_8_study[my_submission_file_8_study['id'].str.contains('_study')]\nprint(study_level.head())\nprint(len(study_level))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.479959Z","iopub.status.idle":"2021-08-15T18:22:46.480577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for file in tqdm(range(len(study_level))):\n    my_row = study_level.loc[file].values.tolist()\n    study_name = my_row[0].split('_')\n\n    complete_name = [x for x in all_files if study_name[0] in x]\n    \n    my_predictions_study = np.zeros((1,4))\n    for images in complete_name:\n        \n        image_data = cv2.imread(os.path.join(rgb_test_images_path, images), -1)\n        if image_data is not None:\n            my_image_8 = cv2.resize(image_data, image_size_8[:2])/255.0\n            my_image_expanded_8 = np.zeros((3,*image_size_8))\n            my_image_expanded_8[0,...] = my_image_8\n            my_image_expanded_8[1,...] = np.fliplr(my_image_8)\n            my_image_expanded_8[2,...] = np.roll(my_image_8, 30, axis=1)\n            \n            my_predictions_8 = my_model_8.predict(my_image_expanded_8)\n            \n            #try:\n            #    index = np.argmax(my_predictions_8[:,statistics.mode(np.argmax(my_predictions_8, axis=0))],axis=0)\n            #    my_predictions_8 = np.reshape(my_predictions_8[index,:], (1,4))\n            #except:\n            my_predictions_8 = np.reshape(np.mean(my_predictions_8, axis=0), (1,4))\n            \n            del my_image_8\n            del my_image_expanded_8\n            \n            my_predictions_study = np.concatenate((my_predictions_study, my_predictions_8))\n            \n            #my_prediction_image = np.mean(np.vstack((my_predictions_1)), axis=0)\n            image_four_class_classification_model_8[images.split('_')[-1].split('.')[0]+'_image'] = my_predictions_8\n            \n            \n    if len(my_predictions_study) > 1:\n        my_predictions_study = list(my_predictions_study[1,:])\n\n\n        prediction_string = 'negative' + ' ' + str(my_predictions_study[0]) + ' 0 0 1 1 ' + 'typical' + ' ' + str(my_predictions_study[1]) + ' 0 0 1 1 ' + 'indeterminate' + ' ' + str(my_predictions_study[2]) + ' 0 0 1 1 ' + 'atypical' + ' ' + str(my_predictions_study[3]) + ' 0 0 1 1' \n\n        #print(prediction_string)\n\n        search_name = study_name[0] + '_study'\n        my_submission_file_8_study['PredictionString'][my_submission_file_8_study.id==search_name] = prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.481730Z","iopub.status.idle":"2021-08-15T18:22:46.482357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del my_model_8\ngc.collect()\n\nmy_submission_file_3_study","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.483520Z","iopub.status.idle":"2021-08-15T18:22:46.484126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_model_9 = tf.keras.models.load_model('../input/siim-effnetv2-keras-study-train-tpu-cv0-805/model3_ver16.h5')\nmy_model_9 = tf.keras.Model(inputs=my_model_9.input, outputs=my_model_9.get_layer('Classification_Head').output)\nprint('Model 9 Loaded')\n\nstudy_level = my_submission_file_9_study[my_submission_file_9_study['id'].str.contains('_study')]\nprint(study_level.head())\nprint(len(study_level))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.485278Z","iopub.status.idle":"2021-08-15T18:22:46.485927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for file in tqdm(range(len(study_level))):\n    my_row = study_level.loc[file].values.tolist()\n    study_name = my_row[0].split('_')\n\n    complete_name = [x for x in all_files if study_name[0] in x]\n    \n    my_predictions_study = np.zeros((1,4))\n    for images in complete_name:\n        \n        image_data = cv2.imread(os.path.join(rgb_test_images_path, images), -1)\n        if image_data is not None:\n            my_image_9 = cv2.resize(image_data, image_size_9[:2])/255.0\n            my_image_expanded_9 = np.zeros((3,*image_size_9))\n            my_image_expanded_9[0,...] = my_image_9\n            my_image_expanded_9[1,...] = np.fliplr(my_image_9)\n            my_image_expanded_9[2,...] = np.roll(my_image_9, 30, axis=1)\n            \n            my_predictions_9 = my_model_9.predict(my_image_expanded_9)\n            \n            #try:\n            #    index = np.argmax(my_predictions_9[:,statistics.mode(np.argmax(my_predictions_9, axis=0))],axis=0)\n            #    my_predictions_9 = np.reshape(my_predictions_9[index,:], (1,4))\n            #except:\n            my_predictions_9 = np.reshape(np.mean(my_predictions_9, axis=0), (1,4))\n\n            del my_image_9\n            del my_image_expanded_9\n            \n            my_predictions_study = np.concatenate((my_predictions_study, my_predictions_9))\n            \n            #my_prediction_image = np.mean(np.vstack((my_predictions_1)), axis=0)\n            image_four_class_classification_model_9[images.split('_')[-1].split('.')[0]+'_image'] = my_predictions_9\n            \n            \n    if len(my_predictions_study) > 1:\n        my_predictions_study = list(my_predictions_study[1,:])\n\n\n        prediction_string = 'negative' + ' ' + str(my_predictions_study[0]) + ' 0 0 1 1 ' + 'typical' + ' ' + str(my_predictions_study[1]) + ' 0 0 1 1 ' + 'indeterminate' + ' ' + str(my_predictions_study[2]) + ' 0 0 1 1 ' + 'atypical' + ' ' + str(my_predictions_study[3]) + ' 0 0 1 1' \n\n        #print(prediction_string)\n\n        search_name = study_name[0] + '_study'\n        my_submission_file_9_study['PredictionString'][my_submission_file_9_study.id==search_name] = prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.487225Z","iopub.status.idle":"2021-08-15T18:22:46.487871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del my_model_9\ngc.collect()\n\nmy_submission_file_9_study","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.489649Z","iopub.status.idle":"2021-08-15T18:22:46.490303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_model_10 = tf.keras.models.load_model('../input/siim-effnetv2-keras-study-train-tpu-cv0-805/model4_ver16.h5')\nmy_model_10 = tf.keras.Model(inputs=my_model_10.input, outputs=my_model_10.get_layer('Classification_Head').output)\nprint('Model 10 Loaded')\n\nstudy_level = my_submission_file_10_study[my_submission_file_10_study['id'].str.contains('_study')]\nprint(study_level.head())\nprint(len(study_level))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.491472Z","iopub.status.idle":"2021-08-15T18:22:46.492081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for file in tqdm(range(len(study_level))):\n    my_row = study_level.loc[file].values.tolist()\n    study_name = my_row[0].split('_')\n\n    complete_name = [x for x in all_files if study_name[0] in x]\n    \n    my_predictions_study = np.zeros((1,4))\n    for images in complete_name:\n        \n        image_data = cv2.imread(os.path.join(rgb_test_images_path, images), -1)\n        if image_data is not None:\n            my_image_10 = cv2.resize(image_data, image_size_10[:2])/255.0\n            my_image_expanded_10 = np.zeros((3,*image_size_10))\n            my_image_expanded_10[0,...] = my_image_10\n            my_image_expanded_10[1,...] = np.fliplr(my_image_10)\n            my_image_expanded_10[2,...] = np.roll(my_image_10, 30, axis=1)\n            \n            my_predictions_10 = my_model_10.predict(my_image_expanded_10)\n            \n            #try:\n            #    index = np.argmax(my_predictions_10[:,statistics.mode(np.argmax(my_predictions_10, axis=0))],axis=0)\n            #    my_predictions_10 = np.reshape(my_predictions_10[index,:], (1,4))\n            #except:\n            my_predictions_10 = np.reshape(np.mean(my_predictions_10, axis=0), (1,4))\n            \n            del my_image_10\n            del my_image_expanded_10\n            \n            my_predictions_study = np.concatenate((my_predictions_study, my_predictions_10))\n            \n            #my_prediction_image = np.mean(np.vstack((my_predictions_1)), axis=0)\n            image_four_class_classification_model_10[images.split('_')[-1].split('.')[0]+'_image'] = my_predictions_10\n            \n            \n    if len(my_predictions_study) > 1:\n        my_predictions_study = list(my_predictions_study[1,:])\n\n\n        prediction_string = 'negative' + ' ' + str(my_predictions_study[0]) + ' 0 0 1 1 ' + 'typical' + ' ' + str(my_predictions_study[1]) + ' 0 0 1 1 ' + 'indeterminate' + ' ' + str(my_predictions_study[2]) + ' 0 0 1 1 ' + 'atypical' + ' ' + str(my_predictions_study[3]) + ' 0 0 1 1' \n\n        #print(prediction_string)\n\n        search_name = study_name[0] + '_study'\n        my_submission_file_10_study['PredictionString'][my_submission_file_10_study.id==search_name] = prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.493215Z","iopub.status.idle":"2021-08-15T18:22:46.493837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del my_model_10\ngc.collect()\n\nmy_submission_file_10_study","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.494942Z","iopub.status.idle":"2021-08-15T18:22:46.495569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_predictions_files = [my_submission_file_1_study, my_submission_file_2_study, my_submission_file_3_study, my_submission_file_4_study, my_submission_file_5_study,\n                          my_submission_file_6_study, my_submission_file_7_study, my_submission_file_8_study, my_submission_file_9_study, my_submission_file_10_study]\n#model_predictions_files = [my_submission_file_1_study]\nprint(len(model_predictions_files))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.496688Z","iopub.status.idle":"2021-08-15T18:22:46.497351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission_file = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.498445Z","iopub.status.idle":"2021-08-15T18:22:46.499065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in tqdm(range(len(my_submission_file))):\n    my_row = my_submission_file.iloc[i]\n    if '_study' in my_row.id:\n        temp = np.zeros((1,4))\n        my_preidction_study = None\n        for model_prediction in model_predictions_files:\n            classification = model_prediction.iloc[i].PredictionString\n            #print(classification)\n            classification_parts = classification.split(' ')\n            #print(classification_parts)\n            if len(classification_parts) > 6:\n                temp = np.vstack((temp, np.array([float(classification_parts[1]), float(classification_parts[7]), float(classification_parts[13]), float(classification_parts[19])])))\n            else:\n                temp = np.vstack((temp, np.array([float(classification_parts[1]), 0, 0, 0])))\n                \n        #print(temp)\n        #my_predictions_study = list(np.mean(temp[1:11,:], axis=0))\n        my_predictions_study = list(np.average(temp[1:11,:], axis=0, weights=[0.11061947, 0.14159292, 0.13274336, 0.10176991, 0.04424779, 0.04424779, 0.11061947, 0.10619469, 0.10619469, 0.10176991]))\n        \n        prediction_string = 'negative' + ' ' + str(my_predictions_study[0]) + ' 0 0 1 1 ' + 'typical' + ' ' + str(my_predictions_study[1]) + ' 0 0 1 1 ' + 'indeterminate' + ' ' + str(my_predictions_study[2]) + ' 0 0 1 1 ' + 'atypical' + ' ' + str(my_predictions_study[3]) + ' 0 0 1 1' \n        \n    \n        my_submission_file['PredictionString'][my_submission_file.id==my_row.id] = prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.500200Z","iopub.status.idle":"2021-08-15T18:22:46.500846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission_file","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.501979Z","iopub.status.idle":"2021-08-15T18:22:46.502612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Image Level Localisation**","metadata":{}},{"cell_type":"code","source":"my_submission_file_1_image = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\nmy_submission_file_2_image = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\nmy_submission_file_3_image = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\nmy_submission_file_4_image = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\nmy_submission_file_5_image = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\nmy_submission_file_6_image = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.503706Z","iopub.status.idle":"2021-08-15T18:22:46.504333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(os.listdir('./test_images_rgb/')))\nprint(len(os.listdir('./test_images_psuedo_color/')))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.505396Z","iopub.status.idle":"2021-08-15T18:22:46.505991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python ../input/yolov5/yolov5/yolov5/detect.py --weights '../input/yolov5/weights_1/best.pt' --source './test_images_psuedo_color/' --img 640 --conf 0.001 --iou-thres 0.5  --save-txt --save-conf --nosave --project './model_1_outputs/'  --augment","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.507130Z","iopub.status.idle":"2021-08-15T18:22:46.507762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotation_files = os.listdir(os.path.join(model_1_outputs_path, 'exp', 'labels'))\nannotation_files = [x for x in annotation_files if '.txt' in x]\nprint(len(annotation_files))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.508926Z","iopub.status.idle":"2021-08-15T18:22:46.509544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = ['lung opacity']\n\nimage_pred_strings = []\nfor file in tqdm(annotation_files):\n    \n    search_name = file.split('_')[-1].split('.')[0] + '_image'\n    confidence, bboxes = get_conf_bboxes(os.path.join(model_1_outputs_path, 'exp', 'labels', file))\n    #print(image_height_width[search_name][0],image_height_width[search_name][1])\n    bboxes = correct_bbox_format(bboxes,image_height_width[search_name][0],image_height_width[search_name][1])\n    \n    prediction_string = ''\n    \n    for j, conf in enumerate(confidence):\n        prediction_string += f'opacity {conf} ' + ' '.join(map(str, bboxes[j])) + ' '        \n    \n    prediction_string = prediction_string[:-1] \n    \n    \n    #print(prediction_string)\n\n    prediction_string = bounding_box_sorter(search_name,prediction_string)\n    my_submission_file_1_image['PredictionString'][my_submission_file_1_image.id==search_name] = prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.510654Z","iopub.status.idle":"2021-08-15T18:22:46.511322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python ../input/yolov5/yolov5/yolov5/detect.py --weights '../input/yolov5/weights_2/best.pt' --source './test_images_psuedo_color/' --img 1088 --conf 0.001 --iou-thres 0.5  --save-txt --save-conf --nosave --project './model_2_outputs/' --augment","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.512406Z","iopub.status.idle":"2021-08-15T18:22:46.513031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotation_files = os.listdir(os.path.join(model_2_outputs_path, 'exp', 'labels'))\nannotation_files = [x for x in annotation_files if '.txt' in x]\nprint(len(annotation_files))\n#print(annotation_files)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.514163Z","iopub.status.idle":"2021-08-15T18:22:46.514820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = ['lung opacity']\n\nimage_pred_strings = []\nfor file in tqdm(annotation_files):\n    \n    search_name = file.split('_')[-1].split('.')[0] + '_image'\n    confidence, bboxes = get_conf_bboxes(os.path.join(model_2_outputs_path, 'exp', 'labels', file))\n    #print(image_height_width[search_name][0],image_height_width[search_name][1])\n    bboxes = correct_bbox_format(bboxes, image_height_width[search_name][0],image_height_width[search_name][1])\n    \n    prediction_string = ''\n    \n    for j, conf in enumerate(confidence):\n        prediction_string += f'opacity {conf} ' + ' '.join(map(str, bboxes[j])) + ' '        \n    \n    prediction_string = prediction_string[:-1] \n    \n    \n    #print(prediction_string)\n\n    prediction_string = bounding_box_sorter(search_name,prediction_string)\n    my_submission_file_2_image['PredictionString'][my_submission_file_2_image.id==search_name] = prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.516016Z","iopub.status.idle":"2021-08-15T18:22:46.516715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python ../input/yolov5/yolov5/yolov5/detect.py --weights '../input/yolov5/weights_3/best.pt' --source './test_images_rgb/' --img 640 --conf 0.001 --iou-thres 0.5  --save-txt --save-conf --nosave --project './model_3_outputs/'  --augment","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.517899Z","iopub.status.idle":"2021-08-15T18:22:46.518521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotation_files = os.listdir(os.path.join(model_3_outputs_path, 'exp', 'labels'))\nannotation_files = [x for x in annotation_files if '.txt' in x]\nprint(len(annotation_files))\n#print(annotation_files)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.519597Z","iopub.status.idle":"2021-08-15T18:22:46.520198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = ['lung opacity']\n\nimage_pred_strings = []\nfor file in tqdm(annotation_files):\n    \n    search_name = file.split('_')[-1].split('.')[0] + '_image'\n    confidence, bboxes = get_conf_bboxes(os.path.join(model_3_outputs_path, 'exp', 'labels', file))\n    #print(image_height_width[search_name][0],image_height_width[search_name][1])\n    bboxes = correct_bbox_format(bboxes, image_height_width[search_name][0],image_height_width[search_name][1])\n    \n    prediction_string = ''\n    \n    for j, conf in enumerate(confidence):\n        prediction_string += f'opacity {conf} ' + ' '.join(map(str, bboxes[j])) + ' '        \n    \n    prediction_string = prediction_string[:-1] \n    \n    \n    #print(prediction_string)\n\n    prediction_string = bounding_box_sorter(search_name,prediction_string)\n    my_submission_file_3_image['PredictionString'][my_submission_file_3_image.id==search_name] = prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.521425Z","iopub.status.idle":"2021-08-15T18:22:46.521980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python ../input/yolov5/yolov5/yolov5/detect.py --weights '../input/yolov5/weights_4/best.pt' --source './test_images_psuedo_color/' --img 640 --conf 0.001 --iou-thres 0.5  --save-txt --save-conf --nosave --project './model_4_outputs/' --augment","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.523068Z","iopub.status.idle":"2021-08-15T18:22:46.523713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotation_files = os.listdir(os.path.join(model_4_outputs_path, 'exp', 'labels'))\nannotation_files = [x for x in annotation_files if '.txt' in x]\nprint(len(annotation_files))\n#print(annotation_files)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.524794Z","iopub.status.idle":"2021-08-15T18:22:46.525406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = ['lung opacity']\n\nimage_pred_strings = []\nfor file in tqdm(annotation_files):\n    \n    search_name = file.split('_')[-1].split('.')[0] + '_image'\n    confidence, bboxes = get_conf_bboxes(os.path.join(model_4_outputs_path, 'exp', 'labels', file))\n    #print(image_height_width[search_name][0],image_height_width[search_name][1])\n    bboxes = correct_bbox_format(bboxes, image_height_width[search_name][0],image_height_width[search_name][1])\n    \n    prediction_string = ''\n    \n    for j, conf in enumerate(confidence):\n        prediction_string += f'opacity {conf} ' + ' '.join(map(str, bboxes[j])) + ' '        \n    \n    prediction_string = prediction_string[:-1] \n    \n    \n    #print(prediction_string)\n\n    prediction_string = bounding_box_sorter(search_name,prediction_string)\n    my_submission_file_4_image['PredictionString'][my_submission_file_4_image.id==search_name] = prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.526793Z","iopub.status.idle":"2021-08-15T18:22:46.527437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This one is to be ignored****","metadata":{}},{"cell_type":"code","source":"!python ../input/yolov5/yolov5/yolov5/detect.py --weights '../input/yolov5/weights_9/best.pt' --source './test_images_psuedo_color/' --img 1088 --conf 0.001 --iou-thres 0.5  --save-txt --save-conf --nosave --project './model_5_outputs/'  --augment","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.528565Z","iopub.status.idle":"2021-08-15T18:22:46.529176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotation_files = os.listdir(os.path.join(model_5_outputs_path, 'exp', 'labels'))\nannotation_files = [x for x in annotation_files if '.txt' in x]\nprint(len(annotation_files))\n#print(annotation_files)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.530302Z","iopub.status.idle":"2021-08-15T18:22:46.530901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = ['lung opacity']\n\nimage_pred_strings = []\nfor file in tqdm(annotation_files):\n    \n    search_name = file.split('_')[-1].split('.')[0] + '_image'\n    confidence, bboxes = get_conf_bboxes(os.path.join(model_5_outputs_path, 'exp', 'labels', file))\n    #print(image_height_width[search_name][0],image_height_width[search_name][1])\n    bboxes = correct_bbox_format(bboxes, image_height_width[search_name][0],image_height_width[search_name][1])\n    \n    prediction_string = ''\n    \n    for j, conf in enumerate(confidence):\n        prediction_string += f'opacity {conf} ' + ' '.join(map(str, bboxes[j])) + ' '        \n    \n    prediction_string = prediction_string[:-1] \n    \n    \n    #print(prediction_string)\n\n    prediction_string = bounding_box_sorter(search_name,prediction_string)\n    my_submission_file_5_image['PredictionString'][my_submission_file_5_image.id==search_name] = prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.532027Z","iopub.status.idle":"2021-08-15T18:22:46.532649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python ../input/yolov5/yolov5/yolov5/detect.py --weights '../input/yolov5/best.pt' --source './test_images_rgb/' --img 512 --conf 0.001 --iou-thres 0.5  --save-txt --save-conf --nosave --project './model_6_outputs/'  --augment","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.533711Z","iopub.status.idle":"2021-08-15T18:22:46.534338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotation_files = os.listdir(os.path.join(model_6_outputs_path, 'exp', 'labels'))\nannotation_files = [x for x in annotation_files if '.txt' in x]\nprint(len(annotation_files))\n#print(annotation_files)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.535526Z","iopub.status.idle":"2021-08-15T18:22:46.536123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = ['lung opacity']\n\nimage_pred_strings = []\nfor file in tqdm(annotation_files):\n    \n    search_name = file.split('_')[-1].split('.')[0] + '_image'\n    confidence, bboxes = get_conf_bboxes(os.path.join(model_6_outputs_path, 'exp', 'labels', file))\n    #print(image_height_width[search_name][0],image_height_width[search_name][1])\n    bboxes = correct_bbox_format(bboxes, image_height_width[search_name][0],image_height_width[search_name][1])\n    \n    prediction_string = ''\n    \n    for j, conf in enumerate(confidence):\n        prediction_string += f'opacity {conf} ' + ' '.join(map(str, bboxes[j])) + ' '        \n    \n    prediction_string = prediction_string[:-1] \n    \n    \n    #print(prediction_string)\n\n    prediction_string = bounding_box_sorter(search_name,prediction_string)\n    my_submission_file_6_image['PredictionString'][my_submission_file_6_image.id==search_name] = prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.537260Z","iopub.status.idle":"2021-08-15T18:22:46.537870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_predictions_file = [my_submission_file_1_image, my_submission_file_2_image, my_submission_file_3_image, my_submission_file_4_image, my_submission_file_5_image, my_submission_file_6_image]\n#models_predictions_file = [my_submission_file_1_image]\nmy_keys = list(image_height_width.keys())\n\nfor i in tqdm(range(len(my_submission_file))):\n    my_row = my_submission_file.iloc[i]\n    if '_image' in my_row.id:\n        boxes_list = []\n        scores_list = []\n        labels_list = []\n        weights = []\n        boxes = []\n        iou_thr = 0.55\n        skip_box_thr = 0.0013\n        sigma = 0.1\n        if my_row.id in my_keys:\n            image_height, image_width = image_height_width[my_row.id][0],image_height_width[my_row.id][1]\n            how_many_predictions = 0\n            for counter, model in enumerate(models_predictions_file):\n                current_model_predictions = model['PredictionString'][model.id==my_row.id].values\n                current_model_predictions = current_model_predictions[0].split(' ')\n                #print(counter)\n                #print(current_model_predictions)\n                if 'none' not in current_model_predictions:\n                    how_many_predictions +=1\n                    temp_boxes = []\n                    temp_scores = []\n                    temp_labels = []\n\n                    for i in range(0,len(current_model_predictions), 6):\n                        current_annotation = current_model_predictions[i:i+6]\n                        temp_scores.append(float(current_annotation[1]))\n                        temp_labels.append(0)\n                        bounding_box = [float(x) for x in current_annotation[2:]]\n                        #print(bounding_box)\n                        bounding_box[0], bounding_box[1], bounding_box[2], bounding_box[3] = (bounding_box[0]-1)/image_width, (bounding_box[1]-1)/image_height, (bounding_box[2]-1)/image_width, (bounding_box[3]-1)/image_height\n                        temp_boxes.append(bounding_box)\n\n                    boxes_list.append(temp_boxes)\n                    scores_list.append(temp_scores)\n                    labels_list.append(temp_labels)\n\n\n\n            if how_many_predictions == 1:\n                weights = [1]\n                boxes, scores, labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n                boxes = denormalise(boxes, image_height, image_width)\n\n            elif how_many_predictions >1:\n                weights = [1, 1, 1, 1, 1, 1.25]\n                boxes, scores, labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n                boxes = denormalise(boxes, image_height, image_width)\n\n\n\n            if len(boxes)>0:\n\n                prediction_string = ''\n\n                for i in range(len(boxes)):\n                    prediction_string += 'opacity ' + str(scores[i]) + ' ' + str(int(boxes[i][0])) + ' ' + str(int(boxes[i][1])) + ' ' + str(int(boxes[i][2])) + ' '  + str(int(boxes[i][3])) + ' '\n\n                prediction_string=prediction_string[:-1]\n                #print(prediction_string)\n\n                prediction_string = bounding_box_sorter(my_row.id,prediction_string)\n                #prediction_string = keep_n_top_loaclisations(prediction_string, 49)\n                my_submission_file['PredictionString'][my_submission_file.id==my_row.id] = prediction_string","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.539313Z","iopub.status.idle":"2021-08-15T18:22:46.539977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission_file","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.541213Z","iopub.status.idle":"2021-08-15T18:22:46.541854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2-Class Classification","metadata":{}},{"cell_type":"code","source":"model_predictions_files = [image_four_class_classification_model_1, image_four_class_classification_model_2, image_four_class_classification_model_3, image_four_class_classification_model_4, image_four_class_classification_model_5,\n                            image_four_class_classification_model_6, image_four_class_classification_model_7, image_four_class_classification_model_8, image_four_class_classification_model_9, image_four_class_classification_model_10]\n\n#model_predictions_files = [image_four_class_classification_model_1, image_four_class_classification_model_2, image_four_class_classification_model_3,\n#                           image_four_class_classification_model_4, image_four_class_classification_model_5]\n\n#model_predictions_files = [my_submission_file_1_study]\nprint(len(model_predictions_files))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.543360Z","iopub.status.idle":"2021-08-15T18:22:46.543963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_keys = image_four_class_classification_model_1.keys()\nfor i in tqdm(range(len(my_submission_file))):\n    my_row = my_submission_file.iloc[i]\n    if '_image' in my_row.id:\n        if my_row.id in all_keys:\n            search_name = [x for x in all_keys if my_row.id in x]\n\n            temp = np.zeros((1,4))\n            for model_predictions in model_predictions_files:\n                temp = np.vstack((temp, model_predictions[search_name[0]]))\n\n            #my_predictions_combined = np.mean(temp, axis=0)[0]\n            my_predictions_combined = np.average(temp[1:11,:], axis=0, weights=[0.11061947, 0.14159292, 0.13274336, 0.10176991, 0.04424779, 0.04424779, 0.11061947, 0.10619469, 0.10619469, 0.10176991])[0]\n            #print(my_predictions_combined)\n\n            current_annotation = my_submission_file['PredictionString'][my_submission_file.id==search_name[0]].values[0]\n            if 'none' in current_annotation:\n                prediction_string = 'none ' + str(my_predictions_combined) + ' 0 0 1 1'\n                my_submission_file['PredictionString'][my_submission_file.id==search_name[0]] = prediction_string\n            else:\n                current_annotation += ' none ' + str(my_predictions_combined) + ' 0 0 1 1'\n                my_submission_file['PredictionString'][my_submission_file.id==search_name[0]] = current_annotation","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.545588Z","iopub.status.idle":"2021-08-15T18:22:46.546212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission_file","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.547626Z","iopub.status.idle":"2021-08-15T18:22:46.548261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Removing Some Particular Classification/Localisation","metadata":{}},{"cell_type":"code","source":"for i in tqdm(range(len(my_submission_file))):\n    my_row = my_submission_file.iloc[i]\n    if '_study' in my_row.id:\n        my_submission_file['PredictionString'][my_submission_file.id==my_row.id] = 'negative 1 0 0 1 1'\n        ","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.549699Z","iopub.status.idle":"2021-08-15T18:22:46.550390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission_file","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.556002Z","iopub.status.idle":"2021-08-15T18:22:46.556595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission_file.to_csv(os.path.join('./', 'submission.csv'), index=False, sep=',')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.558033Z","iopub.status.idle":"2021-08-15T18:22:46.558630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf ./test_images_psuedo_color\n!rm -rf ./test_images_rgb\n!rm -rf ./model_1_outputs\n!rm -rf ./model_2_outputs\n!rm -rf ./model_3_outputs\n!rm -rf ./model_4_outputs\n!rm -rf ./model_5_outputs\n!rm -rf ./model_6_outputs","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:22:46.560059Z","iopub.status.idle":"2021-08-15T18:22:46.560657Z"},"trusted":true},"execution_count":null,"outputs":[]}]}